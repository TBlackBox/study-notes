大家都知道，数据结构与算法解决的主要问题就是“快”和“省”的问题，即如何让代码运行得更快， 如何让代码更节省存储空间。

所以，“快”和“省”是衡量一个算法非常重要的两项指标，也就是我们经常听到的时间复杂度和空间复杂度分析。

那么，为什么需要复杂度分析呢？复杂度分析的方法论是什么呢？

这就是我们本节要解决的问题。

好了，进入今天的学习吧。

# 为什么需要复杂度分析？

首先，我们来思考一个问题：对于两个算法，我们如何评判谁运行得更快，谁运行时更节省内存？

你可能会说，这还不简单，把这两个算法运行一遍，统计下运行时间和占用内存不就可以了吗？

没错，这确实是一种不错的方法，而且它还有个非常形象的名字：**事后统计法**。

但是，这种统计方法具有非常明显的问题：

1. 不同的输入对结果影响很大

   对于一些输入，可能算法A执行得更快；对于另外一些输入，可能算法B执行得更快。比如，我们后面要学习的排序算法，输入的有序性对于不同的排序算法的影响是完全不同的。

2. 不同的机器对结果影响很大

   对于同样的输入，可能在一台机器上算法A更快，而在另外一台机器上算法B更快。比如，算法A可以利用多核而算法B不能，那么CPU的核数对这两个算法的影响将截然不同。

3. 数据规模对结果影响很大

   当数据规模小时，可能算法A更快，而数据规模变大时，可能算法B更快。比如，我们后面要学习的排序算法，当数据规模比较小时，插入排序反而比归并排序更快。

所以，我们需要一种可以不用实际运行算法，就可以估计算法执行效率的方法。

这也就是我们所说的复杂度分析。

那么，怎么进行复杂度分析呢？有没有什么方法论呢？

还真有，这个方法论叫作**渐近分析法**。

# 什么是渐近分析法？

渐近分析法，是指将算法执行的效率与输入的规模进行挂钩，随着输入规模的增大，算法执行所需要的时间（或空间）将呈现一种什么样的趋势，这种趋势就叫作**渐近**，而这种方法就叫作**渐近分析法**。

概念可能比较拗口，我举个简单的例子，对于给定的一个有序数组，我要查找其中某个值所在的位置，比如，查找8这个元素，有哪些方法呢？

![file](../sources\datastructure\1648938-20200721070952640-935718909.jpg)

简单暴力点的方法，从头遍历，查找到该元素即返回。

![file](../sources\datastructure\1648938-20200721070952940-853937590.jpg)

更友好一点的方法，采用二分法，每次定位到数据的中间位置，看其值与目标值的大小，判断是在左边还是右边继续以二分的方式查找。

![file](../sources\datastructure\1648938-20200721070953145-1419684349.jpg)

上面我们举的例子的输入规模是8个元素的有序数组，目标值为8，使用第二种方法明显比第一种方法要快很多。

但是，如果查找的目标是1呢？

对于第一种方法，查找一次足矣。

对于第二种方法，需要查找3次。

此时，第二种方法又次于第一种方法了。

所以，比较两个算法的执行效率，不能只考虑到个别元素，而应该顾及到所有元素的感受。

我们以数学的方法来统计两种方法的平均执行效率，假设输入规模扩展到n。

对于第一种方法，1号元素查找一次，2号元素查找两次，3号元素查找三次……，而查找每个元素的概率都是1/n。

所以，它的执行效率为：1x1/n + 2x1/n + 3x1/n + ... nx1/n = nx(n+1)/2/n = (n+1)/2。

对于第二种方法，中间的元素有一个，查找一次，次中间的元素有两个，查找两次，次次中间的元素有四个，查找三次...，每次查找的规模都缩小一半，而查找每个元素的概率都是1/n。

所以，它的执行效率为：1x1x1/n + 2x2x1/n + 4x3x1/n + ... + 2^(log2(n)-2) x (log2(n)-1) x 1/n+ 2^(log2(n)-1) x log2(n) x 1/n = ?

我了个去，这个结果等于多少？

是时候展现真正的实力了：

![file](../sources\datastructure\1648938-20200721070953327-1606872203.jpg)

你可能要骂娘了，对于我一个小学毕业的，难道我没办法学习数据结构与算法了？

No，No，No，肯定不能这么玩，那么，应该怎么玩呢？我们下一节接着讲。



上面，我们从算法执行效率方面阐述了为什么需要复杂度分析，并介绍了复杂度分析的方法，即渐近分析法，如果严格地遵循渐近分析法，需要大量的数学知识，这无疑增加了我们分析算法的难度，那么，有没有什么更省心地计算复杂度的方法呢？



但是，如果遵循严格的渐近分析法，需要掌握大量数学知识，这无疑给我们评估算法的优劣带来了很大的挑战。

那么，有没有更好地评估算法的方法呢？

答案是必然的，本节，我们就从最坏、平均、最好三种情况来分析分析复杂度。

# 案例

为了便于讲解，我写了一个小例子：

```java
public class LinearSearch {
    public static void main(String[] args) {
        int[] array = new int[]{1, 8, 9, 3, 5, 6, 10, 13};
        int index = search(array, 10);
        System.out.println("index=" + index);
    }

    private static int search(int[] array, int value) {
        for (int i = 0; i < array.length; i++) {
            if (array[i] == value) {
                return i;
            }
        }
        return -1;
    }
}
```

这个例子使用线性搜索从一个数组中查找一个元素，这个元素有可能存在，也有可能不存在于数组中。

# 最坏情况

在最坏情况下，要查找的元素不存在于数组中，此时，它的时间复杂度是多少呢？

很简单，必然需要遍历完所有元素才会发现要查找的元素不存在于数组中。

所以，最坏情况下，使用线性查找的时间复杂度为O(n)。

# 平均情况

在平均情况下，我们要照顾到每一个元素，此时，它的时间复杂度如何计算呢？

在上面，我们已经讲过计算方式了，不过，这里考虑到有元素不存在于数组中，所以，是(n+1)种可能：

```java
1*1/(n+1) + 2*/(n+1) + ... + n*1/(n+1) + (n+1)/(n+1) = 1/(n+1) * (n+1)(n+2)/2 = (n+2)/2
```

所以，在平均情况下，忽略掉常数项，使用线性查找的时间复杂度也是O(n)。

> 为什么要忽略掉常数项？
>
> 当n趋向于无穷大的时候，常数项的意义不是很大，所以，可以忽略，比如(n+2)/2=n/2 + 1，n本身已经趋向于无穷大，加不加1有什么意义呢，n的倍数是2还是1/2并不会有明显的差别。
>
> 同样地，低阶项一般也会抹掉，比如2n^2 + 3n + 1，当n趋向于无穷大的时候，n^2的值是远远大于3n的，所以，不需要保留3n。
>
> 所以，计算复杂度时通常都会把常数项和低阶项抹掉，只保留高阶项。

# 最好情况

最好情况是什么呢？

如果我们要查找的元素正好是数组的第一个元素，查找一次就找到了，这无疑是最好的情况。

所以，在最好情况下，使用线性查找的时间复杂度是O(1)。

# 小结

通过上面的分析，可以看到，最坏情况和最好情况是比较好评估的，而平均情况则比较难以计算。

但是，最好情况又不能代表大多数样本，且平均情况与最坏情况在省略常数项的情况下往往是比较接近的。

所以，通常，我们使用最坏情况来评估算法的时间复杂度，这也是比较简单的一种评估方法，且往往也是比较准确的。

# 后记

本节，我们从最坏、平均、最好三种情况分析了线性查找的时间复杂度，经过详细地分析，我们得出结论，通常使用最坏情况来评估算法的时间复杂度。

请注意，我们这里使用了“通常”，说明有些情况是不能使用最坏情况来评估算法的时间复杂度的。

那么，你知道什么情况下不能使用最坏情况来评估算法的时间复杂度吗？





但是，有些算法是不能使用最坏情况来评估算法的复杂度的。

那么，有哪些算法呢？

本节，我们将从动态数组以及快速排序这两个个例入手来分析不能使用最坏情况评估复杂度的情形。

# 动态数组

动态数组，对应于Java中的ArrayList，在插入元素时，分成两种情况：

1. 数组未满，元素放在size下标的位置即可；
2. 数组满了，需要扩容，一般扩容为N倍大小，Java里面是1.5倍，扩容时需要创建一个新的数组，并把原来的元素一个一个地拷贝到新的数组中，再插入新的元素；

我简单地写一段代码，你可以感受下：

```java
public class DynamicArray {
    private int[] array;
    private int size;

    public DynamicArray(int capacity) {
        this.array = new int[capacity];
        this.size = 0;
    }

    // 插入元素，时间复杂度为多少呢？
    public void add(int element) {
        // 判断是否需要扩容
        if (size >= array.length) {
            int newCapacity = array.length + (array.length >> 1);
            int[] newArray = new int[newCapacity];
            for (int i = 0; i < array.length; i++) {
                newArray[i] = array[i];
            }
            this.array = newArray;
        }
        array[size++] = element;
    }

    public int[] getArray() {
        return array;
    }

    public static void main(String[] args) {
        DynamicArray dynamicArray = new DynamicArray(4);
        dynamicArray.add(1);
        dynamicArray.add(2);
        dynamicArray.add(3);
        dynamicArray.add(4);
        dynamicArray.add(5);
        dynamicArray.add(6);

        for (int element : dynamicArray.getArray()) {
            System.out.println(element);
        }
    }
}
```

那么，对于动态数组，它的插入元素方法的时间复杂度是多少呢？

按照上一节的说法，按照最坏情况来评估，最坏情况是插入元素时正好数组满了需要扩容的时候，此时，需要创建一个额外的数组，同时有一个遍历原数组的过程。

所以，在最坏情况下，动态数组插入元素的时间复杂度为O(n)。

但是，这样合理吗？

显然是不合理的，我插入前面(n-1)个元素的时候，它的时间复杂度都是O(1)，就只有插入第n个元素的时候它的时间复杂度才是O(n)，所以，这样来评估动态数组插入元素的时间复杂度明显不合理。

那么，如果我把第n个元素插入所需要的时间均摊到所有元素上会怎么样呢？

这样的话，前面每个元素的插入时间只需要加1，变成O(2)，忽略常数项，就还是O(1)，这样明显是要合理一些。

这种方式跟计算平均时间复杂度有点类似，但是，它不是平均时间复杂度，它有一个专门的名称叫做**均摊时间复杂度**。

均摊时间复杂度，即对一批样本中出现的个例情况，将它们耗费的时间均摊到所有样本上，算出来的一个时间复杂度。

你可以把它和平均时间复杂度对比一下：

1. 平均时间复杂度的计算中没有个例，所有样本是同等看待的，想一下线性查找的过程；
2. 均摊时间复杂度的计算中有个例，这种个例往往就是最坏的情况，想一下动态数组插入元素的过程；
3. 线性查找第n个元素不是个例，不能把它的时间均摊到所有元素上；

> 这两个概念严格来说是有区别的，如果无法理解，当成一样的也问题不大，比如，这里如果按平均时间复杂度计算的话，结果为 (1+1+1+...+n)/n = (n-1+n)/n = (2n-1)/n=2-1/n，忽略常数项和低阶项，最终的结果也是O(1)。

好了，那么，我们再来看一下动态数组插入元素时的额外空间复杂度。

是不是一样的道理？数组未满时额外空间复杂度为O(1)，数组满时额外空间复杂度为O(n)，均摊一下变成O(1)。

所以，对于动态数组插入元素的过程，它的均摊时间复杂度和均摊额外空间复杂度都是O(1)。

# 快速排序

大家都知道经典的快速排序的时间复杂度是O(nlogn)，那么，它的最坏时间复杂度是不是也是O(nlogn)呢？

让我们来看下面这个数组：

![file](../sources\datastructure\1648938-20200723074403789-1189838235.jpg)

这是一个有序数组，如果此时用经典快速排序来对其进行排序会怎样呢？

我们取最右边的元素为轴（Pivot），也就是12，将小于12的放在它的左边，大于12的放在它的右边，发现没有比12大的，所以，右边没有元素，经过此步，12的位置固定不变了。

接着，将12左右两边的元素再各取最右边的元素为轴，12的右边没有元素，所以，只需要处理左边就可以了，以10为轴，比10小的放在它的左边，比10大的放在它的右边，发现10的右边也没有元素（12已经固定了），经过此步，10的位置固定了。

同样地，最后一步到1这里，排序完成。

让我们来分析一下整个过程的复杂度：

第一步，需要遍历(n-1)个元素；

第二步，需要遍历(n-2)个元素；

...

最后一步，需要遍历0个元素；

这种情况下的时间复杂度为：(n-1) + (n-2) + ... + 1 + 0 = (n-1)n/2 = n^2/2 - n/2，忽略常数项和低阶项，它的时间复杂度为O(n^2)。

所以，对于有序数组，使用经典快速排序，它的时间复杂度为O(n^2)，这也是最坏的情况。

但是，似乎从来没有人告诉你，经典快速排序的时间复杂度为O(n^2)，而是O(nlog2)，这是为什么呢？

那是因为有序数组相对于经典快速排序，也是属于个例，穷举无限多的样本之后，有序数组的可能性实在是太小，所以，我们一般说经典快速排序的时间复杂度为O(nlogn)，而不是以最坏情况来评估它的时间复杂度。

> 我们这里说的是经典快速排序，为什么要加“经典”两个字呢？

# 后记

好了，本节，我们通过两个案例来说明了并不是所有的算法都使用最坏情况来评估它的复杂度。

到现在为止，我们都是使用的大O来表示算法的复杂度，但是，在其它书籍中，你可能还见过Θ、Ω等表示法，它们又是什么意思呢？





前面几节，我们一起学习了算法的复杂度如何分析，并从最坏、平均、最好以及不能使用最坏情况全方位无死角的剖析了算法的复杂度，在我们表示复杂度的时候，通常使用大O来表示。

但是，在其他书籍中，你可能还见过Θ、Ω、o、ω等符号。

那么，这些符号又是什么意思呢？

本节，我们就来解决这个问题。

# 读音

我们先来纠正一波读音：

- O，/əʊ/，大Oh
- o，/əʊ/，小oh
- Θ，/ˈθiːtə/，theta
- Ω，/oʊˈmeɡə/，大Omega
- ω，/oʊˈmeɡə/，小omega

是不是跟老师教得不太一样^^

# 数学解释

## Θ

Θ定义了一种精确的渐近行为（exact asymptotic behavior），怎么说呢？

用函数来表示：

```java
对于f(n)，存在正数n0、c1、c2，使得当 n>=n0 时，始终存在 0 <= c1*g(n) <= f(n) <= c2*g(n)，则我们可以用 f(n)=Θ(g(n))表示。
```

用图来表示：

![file](../sources\datastructure\1648938-20200723230943808-446635436.jpg)

Θ同时定义了上界和下界，f(n)位于上界和下界之间，且包含等号。

比如说，f(n) = 2n^2+3n+1 = Θ(n^2)，此时，g(n)就是用f(n)去掉低阶项和常数项得来的，因为肯定存在某个正数n0、c1、c2，使得 0 <= c1*n^2 <= 2n^2+3n+1 <= c2*n2，当然，你说g(n)是2*n2也没问题，所以，g(n)实际上满足这个条件的一组函数。

好了，如果Θ你能理解了，下面四个就好理解了。

## O

O定义了算法的上界。

用函数来表示：

```java
对于f(n)，存在正数n0、c，使得当 n>=n0 时，始终存在 0 <= f(n) <= c*g(n)，则我们可以用 f(n)=O(g(n))表示。
```

用图来表示：

![file](../sources\datastructure\1648938-20200723230944009-1802922647.jpg)

O只定义上界，只要f(n)不大于c*g(n)，就可以说 f(n)=O(g(n))。

比如说，对于插入排序，我们说它的时间复杂度是O(n^2)，但是，如果用Θ来表示，则必须分成两条：

1. 最坏的情况下，它的时间复杂度为Θ(n^2)；
2. 最好的情况下，它的时间复杂度为Θ(n)。

这里的n2只是g(n)这一组函数中最小的上界，当然，g(n)也可以等于n3。

不过，我们一般说复杂度都是指的最小的上界，比如，这里插入排序的时间复杂度如果说是O(n^3)，从理论上来说，也没问题，只是不符合约定罢了。

> 插入排序最好的情况就是数组本身就是有序的。

## o

o定义的也是算法的上界，不过它不包含等于，是一种不精确的上界，或者称作松上界（某些书籍翻译为非紧上界）。

用函数来表示：

```java
对于f(n)，存在正数n0、c，使得当 n>n0 时，始终存在 0 <= f(n) < c*g(n)，则我们可以用 f(n)=o(g(n))表示。
```

用图来表示：

![file](../sources\datastructure\1648938-20200723230944235-1016006659.jpg)

o表示仅仅是大O去掉等于的情况，其他行为与大O一模一样。

## Ω

Ω定义了算法的下界，与O正好相反。

用函数来表示：

```java
对于f(n)，存在正数n0、c，使得当 n>=n0 时，始终存在 0 <= c*g(n) <= f(n)，则我们可以用 f(n)=Ω(g(n))表示。
```

用图来表示：

![file](../sources\datastructure\1648938-20200723230944470-737988666.jpg)

Ω只定义下界，只要f(n)不小于c*g(n)，就可以说 f(n)=Ω(g(n))。

比如，对于插入排序，我们可以说它的时间复杂度为Ω(n)，不过，这通常没有什么意义，因为插入排序在最好的情况下很少，基本都是在最坏情况或者平均情况。

## ω

ω同样定义的是下界，只不过不包含等于，是一种不精确的下界，或者称作松下界（某些书籍翻译为非紧下界）。

用函数来表示：

```java
对于f(n)，存在正数n0、c，使得当 n>n0 时，始终存在 0 <= c*g(n) < f(n)，则我们可以用 f(n)=ω(g(n))表示。
```

用图来表示：

![file](../sources\datastructure\1648938-20200723230944653-796356770.jpg)

ω表示仅仅是大Ω去掉等于的情况，其他行为与大Ω一模一样。

# 通俗理解

| 符号 | 含义           | 通俗理解   |
| ---- | -------------- | ---------- |
| Θ    | 精确的渐近行为 | 相当于“=”  |
| O    | 上界           | 相当于“<=” |
| o    | 松上界         | 相当于“<”  |
| Ω    | 下界           | 相当于“>=” |
| ω    | 松下界         | 相当于“>”  |

# 小结

为了帮助同学们快速查阅英文资料，彤哥特地把这几节涉及到的英语单词汇总了一下：

| 汉语               | 英文                      |
| ------------------ | ------------------------- |
| 复杂度             | complexity                |
| 时间复杂度         | time complexity           |
| 空间复杂度         | space complexity          |
| 渐近分析           | asymptotic analysis       |
| 最坏情况           | the worst case            |
| 最好情况           | the best case             |
| 平均情况           | the average case          |
| 精确的渐近行为     | exact asymptotic behavior |
| 低阶项             | low order terms           |
| 常数项（前置常数） | leading constants         |
| 松上界             | loose upper-bound         |

# 后记

本节，我们分别从读音、数学、通俗理解等三个方面阐述了Θ、O、o、Ω、ω的含义，并在最后给出了这几节涉及到的术语对应的英文，有了这些英文，你也可以快速地查阅这方面的资料。

不过，在我们平时与人交流的过程中，大家还是习惯于使用大O表示法，一来它表示最坏情况，最坏情况通常可以直接代表算法的复杂度，二来它比较好书写。

所以，我们只需要记住大O就可以了，只不过在别人提到Θ、Ω、ω我们知道是什么含义就可以了。

前面几节讲了这么多，其实，还是只涉及了很简单的算法复杂度。

那么，常见的算法复杂度有哪些呢？



上一节，我们一起学习了表示复杂度的几个符号，我们说，通常使用大O来表示算法的复杂度，不仅合理，而且书写方便。

那么，使用大O表示法评估算法的复杂度有没有什么套路呢？以及常见的复杂度有哪些呢？

本节，我们就来解决这两个问题。

# 前情回顾

在正式讲解套路之前，我们先回忆一下前面几节讲到的内容。

在第2节，我们学习了渐近分析法，将算法的复杂度与输入规模挂钩，随着输入规模的增大，算法执行的时间将呈现一种什么样的趋势，将这个趋势用函数表示，再去除低阶项和常数项，就得到了算法的时间复杂度。

在第3节，我们分别从最坏、平均、最好三种情况来分析了算法的复杂度，得出结论，一般使用最坏情况来评估算法的复杂度。

在第4节，我们通过动态数组的插入元素及经典快速排序的时间复杂度，解释了有的时候不能使用最坏情况来评估算法的复杂度。

在第5节，我们从读音、数学、通俗理解三个方面分析了各种表示算法复杂度的符号，得出结论还是使用大O比较香，大O代表了算法的上界，它与前面讲到的最坏情况往往是对应的。

所以，这里所说的套路也是针对大部分情况，也就是最坏情况，对于一些个例，比如经典快排，我们虽然也是使用大O表示他们的复杂度，但是，其实是一种均摊的复杂度。

好了，让我们看看计算算法复杂度的套路到底是什么吧。

# 套路

我将计算算法复杂度的套路归纳为以下五步：

1. 明确输入规模n；
2. 考虑最坏情况或均摊情况，如果最坏情况为个例，那就是均摊；
3. 计算算法执行的次数与n的关系，并用函数表示出来；
4. 去除低阶项；
5. 去除常数项；

比如，对于在数组中查找指定元素的操作：

1. 输入规模为数组的长度n；
2. 考虑最坏情况为目标元素不在数组中；
3. 算法的执行次数为遍历所有数组元素，也就是n次，用函数表示f(n) = n；
4. 去除低阶项，没有低阶项，还是n；
5. 去除常数项，没有常数项，还是n；

所以，在数组中查找指定元素的时间复杂度为O(n)。

OK，使用这种方式可以很快的计算出算法的复杂度，也不需要进行额外的计算，非常快捷高效。

# 常见的复杂度

上面我们说了，复杂度的计算就是计算与输入规模n的关系，所以，我们想想数学中关于n的函数就能得出常见的复杂度了，我绘制了一张表格：

| 与n的关系      | 英文释义    | 复杂度   | 示例                         |
| -------------- | ----------- | -------- | ---------------------------- |
| 常数（不相关） | Constant    | O(1)     | 数组按索引查找元素           |
| 对数相关       | Logarithmic | O(logn)  | 二分查找                     |
| 线性相关       | Linear      | O(n)     | 遍历数组的元素               |
| 超线性相关     | Superlinear | O(nlogn) | 归并排序、堆排序             |
| 多项式相关     | Polynomial  | O(n^c)   | 冒泡排序、插入排序、选择排序 |
| 指数相关       | Exponential | O(c^n)   | 汉诺塔                       |
| 阶乘相关       | Factorial   | O(n!)    | 行列式展开                   |
| n的n次方       | 无          | O(n^n)   | 不知道有没有这种算法         |

在这张表中，复杂度是依次增加的，可以看到常数复杂度O(1)无疑是最好的，让我们用一张图来直观感受下：

![file](../sources\datastructure\1648938-20200725121018096-1666542836.jpg)

# 后记

本节，我们一起学习了复杂度分析的套路以及常见的复杂度，到目前为止，我们不管是举例还是讲解基本上都在说时间复杂度。

那么，空间复杂度又是什么呢？空间与时间之间如何权衡呢？



上一节，我们一起学习了复杂度分析的套路和常见的复杂度。

但是，我们的案例基本都是以时间复杂度为主，很少接触到空间复杂度。

那么，到底什么才是真正的空间复杂度呢？在空间与时间发生冲突时又该如何权衡呢？

本节，我们就来解决这两个问题。

# 来个例子

现在有一个算法是这样的，给定一个数组，将数组中每个元素都乘以2返回，我实现了下面两种形式：

```java
private static int[] multi1(int[] array) {
    int[] newArray = new int[array.length];
    for (int i = 0; i < array.length; i++) {
        newArray[i] = array[i] * 2;
    }
    return newArray;
}

private static int[] multi2(int[] array) {
    for (int i = 0; i < array.length; i++) {
        array[i] = array[i] * 2;
    }
    return array;
}
```

暂且不论这两个算法孰好孰坏，你来猜猜他们的空间复杂度各是多少？

你可能会说第一个算法的空间复杂度为O(n)，第二个算法的空间复杂度为O(1)。

错！两个算法的空间复杂度都是O(n)。

也不能说你完全错了，因为大部分书籍或者资料都弄错了。

是时候了解真正的空间复杂度了。

# 空间复杂度与额外空间复杂度

**空间复杂度**，是指一个算法运行的过程占用的空间，这个空间包括输入参数的占用空间和额外申请的空间。

所以，针对上面两个算法：

- 第一个算法，输入参数n，额外空间n，两者相加为2n，去除常数项，空间复杂度为O(n)；
- 第二个算法，输入参数n，额外空间0，两者相加为n，空间复杂度为O(n)。

可以看到，使用空间复杂度很难判断这两个算法的好坏，所以，诞生了另一个概念——额外空间复杂度。

**额外空间复杂度**，是指一个算法运行过程中额外申请的空间。

使用额外空间复杂度，针对上面两个算法：

- 第一个算法，额外空间为n，额外空间复杂度为O(n)；
- 第二个算法，额外空间为0，额外空间复杂度为O(1)；

> 似乎没见过有O(0)这种写法。

可以看到，使用额外空间复杂度能够很轻易地判断两个算法的好坏（从空间占用的角度）。

所以，是时候纠正错误的概念了，以后与人交流的时候请使用“额外空间复杂度”这个概念。

# 时间与空间的权衡

时间与空间往往是一组纠缠在一起的概念，就像很多小说中写的一样，主角最终领悟了时空法则，成为了最强者，小说结束。

在数据结构与算法中也是一样，时间与空间往往同时出现，而且经常朝着相反的方向运动。

比如，对于排序算法：

- 冒泡排序，时间复杂度O(n^2)，空间复杂度O(1)
- 归并排序，时间复杂度O(nlogn)，空间复杂度O(n)

所以，有两种思想：以时间换空间，以空间换时间。

那么，哪种算法更好呢？

我认为，如果有时间、空间同时比较小的为最好，退而求其次，我选择以空间换时间，毕竟，随着计算机硬件技术地不断发展，空间越来越不值钱，而时间却越来越值钱，所以，以空间换时间也是一种常用的思想，在我们后续的课程中会出现大量以空间换时间的案例。

> 想知道冒泡排序和归并排序算法的复杂度如何计算吗？来呀，关注我吧。

# 后记

本节，我们从一个小例子入手，分析了两种算法的空间复杂度，并引出空间复杂度的真身——额外空间复杂度，最后，通过对比冒泡排序和归并排序的时间复杂度和空间复杂度，得出了以空间换时间的思想。

到这里，关于复杂度相关的章节就写完了，从下一节开始，我们将进入常用数据结构与算法的学习中，敬请期待。